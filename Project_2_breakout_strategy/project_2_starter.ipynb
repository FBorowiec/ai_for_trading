{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Breakout Strategy\n",
    "## Instructions\n",
    "Each problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment. After implementing the function, run the cell to test it against the unit tests we've provided. For each problem, we provide one or more unit tests from our `project_tests` package. These unit tests won't tell you if your answer is correct, but will warn you of any major errors. Your code will be checked for the correct solution when you submit it to Udacity.\n",
    "\n",
    "## Packages\n",
    "When you implement the functions, you'll only need to you use the packages you've used in the classroom, like [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/). These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n",
    "\n",
    "The other packages that we're importing are `helper`, `project_helper`, and `project_tests`. These are custom packages built to help you solve the problems.  The `helper` and `project_helper` module contains utility functions and graph functions. The `project_tests` contains the unit tests for all the problems.\n",
    "\n",
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colour==0.1.5 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (0.1.5)\n",
      "Collecting cvxpy==1.0.3 (from -r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/59/2613468ffbbe3a818934d06b81b9f4877fe054afbf4f99d2f43f398a0b34/cvxpy-1.0.3.tar.gz (880kB)\n",
      "\u001b[K    100% |████████████████████████████████| 880kB 15.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cycler==0.10.0 in /opt/conda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from -r requirements.txt (line 3)) (0.10.0)\n",
      "Collecting numpy==1.13.3 (from -r requirements.txt (line 4))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/a7/e3e6bd9d595125e1abbe162e323fd2d06f6f6683185294b79cd2cdb190d5/numpy-1.13.3-cp36-cp36m-manylinux1_x86_64.whl (17.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.0MB 2.0MB/s eta 0:00:01  6% |██                              | 1.0MB 6.2MB/s eta 0:00:03    33% |██████████▊                     | 5.7MB 31.0MB/s eta 0:00:01    68% |██████████████████████          | 11.7MB 33.3MB/s eta 0:00:01    77% |████████████████████████▊       | 13.1MB 30.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.21.1 (from -r requirements.txt (line 5))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/e1/6c514df670b887c77838ab856f57783c07e8760f2e3d5939203a39735e0e/pandas-0.21.1-cp36-cp36m-manylinux1_x86_64.whl (26.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 26.2MB 1.6MB/s eta 0:00:01   27% |████████▊                       | 7.2MB 30.1MB/s eta 0:00:01    37% |████████████                    | 9.9MB 29.3MB/s eta 0:00:01    56% |██████████████████              | 14.8MB 25.8MB/s eta 0:00:01    66% |█████████████████████▏          | 17.3MB 27.2MB/s eta 0:00:01    70% |██████████████████████▋         | 18.5MB 22.8MB/s eta 0:00:01    79% |█████████████████████████▍      | 20.8MB 24.4MB/s eta 0:00:01    88% |████████████████████████████▎   | 23.2MB 25.0MB/s eta 0:00:01    95% |██████████████████████████████▋ | 25.1MB 26.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plotly==2.2.3 (from -r requirements.txt (line 6))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/a6/8214b6564bf4ace9bec8a26e7f89832792be582c042c47c912d3201328a0/plotly-2.2.3.tar.gz (1.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.1MB 8.0MB/s eta 0:00:01    95% |██████████████████████████████▋ | 1.0MB 27.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing==2.2.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil==2.6.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (2.6.1)\n",
      "Requirement already satisfied: pytz==2017.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (2017.3)\n",
      "Requirement already satisfied: requests==2.18.4 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 10)) (2.18.4)\n",
      "Collecting scipy==1.0.0 (from -r requirements.txt (line 11))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/5e/caa01ba7be11600b6a9d39265440d7b3be3d69206da887c42bef049521f2/scipy-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (50.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 50.0MB 663kB/s eta 0:00:01  6% |██▏                             | 3.3MB 26.5MB/s eta 0:00:02    11% |███▊                            | 5.8MB 26.7MB/s eta 0:00:02    16% |█████▎                          | 8.2MB 26.2MB/s eta 0:00:02    18% |█████▉                          | 9.2MB 23.8MB/s eta 0:00:02    20% |██████▊                         | 10.5MB 23.0MB/s eta 0:00:02    22% |███████▍                        | 11.5MB 23.3MB/s eta 0:00:02    25% |████████▏                       | 12.8MB 23.1MB/s eta 0:00:02    27% |████████▉                       | 13.8MB 25.1MB/s eta 0:00:02    32% |██████████▎                     | 16.1MB 22.8MB/s eta 0:00:02    34% |███████████                     | 17.3MB 23.8MB/s eta 0:00:02    39% |████████████▌                   | 19.6MB 24.6MB/s eta 0:00:02    40% |█████████████                   | 20.5MB 23.7MB/s eta 0:00:02    45% |██████████████▋                 | 22.8MB 25.0MB/s eta 0:00:02    47% |███████████████▏                | 23.7MB 16.9MB/s eta 0:00:02    50% |████████████████▏               | 25.3MB 25.0MB/s eta 0:00:01    53% |█████████████████               | 26.6MB 36.6MB/s eta 0:00:01    54% |█████████████████▌              | 27.3MB 19.8MB/s eta 0:00:02    56% |██████████████████              | 28.3MB 22.1MB/s eta 0:00:01    58% |██████████████████▊             | 29.3MB 24.6MB/s eta 0:00:01    62% |████████████████████▏           | 31.5MB 22.9MB/s eta 0:00:01    64% |████████████████████▉           | 32.5MB 22.3MB/s eta 0:00:01    67% |█████████████████████▌          | 33.6MB 24.5MB/s eta 0:00:01    73% |███████████████████████▌        | 36.7MB 26.5MB/s eta 0:00:01    75% |████████████████████████▎       | 38.0MB 25.5MB/s eta 0:00:01    82% |██████████████████████████▍     | 41.2MB 24.8MB/s eta 0:00:01    88% |████████████████████████████▍   | 44.4MB 24.2MB/s eta 0:00:01    93% |█████████████████████████████▉  | 46.6MB 33.8MB/s eta 0:00:01    95% |██████████████████████████████▋ | 47.9MB 33.1MB/s eta 0:00:01    98% |███████████████████████████████▌| 49.3MB 28.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn==0.19.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 12)) (0.19.1)\n",
      "Requirement already satisfied: six==1.11.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 13)) (1.11.0)\n",
      "Collecting tqdm==4.19.5 (from -r requirements.txt (line 14))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/3c/341b4fa23cb3abc335207dba057c790f3bb329f6757e1fcd5d347bcf8308/tqdm-4.19.5-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 6.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting zipline==1.2.0 (from -r requirements.txt (line 15))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d3/689f2a940478b82ac57c751a40460598221fd82b0449a7a8f7eef47a3bcc/zipline-1.2.0.tar.gz (659kB)\n",
      "\u001b[K    100% |████████████████████████████████| 665kB 13.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting osqp (from cvxpy==1.0.3->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/01/8becb29b0d38e0c40eab9e3d54aa8138fa62a010d519caf65e9210021bd3/osqp-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (147kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 17.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting ecos>=2 (from cvxpy==1.0.3->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/ed/d131ff51f3a8f73420eb1191345eb49f269f23cadef515172e356018cde3/ecos-2.0.7.post1-cp36-cp36m-manylinux1_x86_64.whl (147kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 14.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting scs>=1.1.3 (from cvxpy==1.0.3->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/6e/dbdd778c64c1920ae357a2013ea655d65a1f8b60f397d6e5549e4aafe8ec/scs-2.1.1-2.tar.gz (157kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 14.1MB/s ta 0:00:01  26% |████████▍                       | 40kB 9.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess (from cvxpy==1.0.3->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/40/b23b1ddd3cb0e072fdbec5f458e8369df48643a3b04dfb55365a63a51687/multiprocess-0.70.8.tar.gz (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 10.4MB/s ta 0:00:01 1% |▍                               | 20kB 3.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fastcache in /opt/conda/lib/python3.6/site-packages (from cvxpy==1.0.3->-r requirements.txt (line 2)) (1.0.2)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.6/site-packages (from cvxpy==1.0.3->-r requirements.txt (line 2)) (0.8.2)\n",
      "Requirement already satisfied: decorator>=4.0.6 in /opt/conda/lib/python3.6/site-packages (from plotly==2.2.3->-r requirements.txt (line 6)) (4.0.11)\n",
      "Requirement already satisfied: nbformat>=4.2 in /opt/conda/lib/python3.6/site-packages (from plotly==2.2.3->-r requirements.txt (line 6)) (4.4.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 10)) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 10)) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 10)) (1.22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 10)) (2017.11.5)\n",
      "Requirement already satisfied: pip>=7.1.0 in /opt/conda/lib/python3.6/site-packages (from zipline==1.2.0->-r requirements.txt (line 15)) (18.1)\n",
      "Requirement already satisfied: setuptools>18.0 in /opt/conda/lib/python3.6/site-packages (from zipline==1.2.0->-r requirements.txt (line 15)) (38.4.0)\n",
      "Collecting Logbook>=0.12.5 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/83/20fc0270614919cb799f76e32cf143a54c58ce2fa45c19fd38ac2e4f9977/Logbook-1.4.3.tar.gz (85kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 11.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting requests-file>=1.4.1 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n",
      "Collecting pandas-datareader<0.6,>=0.2.1 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/c5/cc720f531bbde0efeab940de400d0fcc95e87770a3abcd7f90d6d52a3302/pandas_datareader-0.5.0-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 10.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: patsy>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from zipline==1.2.0->-r requirements.txt (line 15)) (0.4.1)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in /opt/conda/lib/python3.6/site-packages (from zipline==1.2.0->-r requirements.txt (line 15)) (0.8.0)\n",
      "Requirement already satisfied: Cython>=0.25.2 in /opt/conda/lib/python3.6/site-packages (from zipline==1.2.0->-r requirements.txt (line 15)) (0.29.7)\n",
      "Collecting cyordereddict>=0.2.2 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/1a/364cbfd927be1b743c7f0a985a7f1f7e8a51469619f9fefe4ee9240ba210/cyordereddict-1.0.0.tar.gz (138kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 14.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting bottleneck>=1.0.0 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/ae/cedf5323f398ab4e4ff92d6c431a3e1c6a186f9b41ab3e8258dff786a290/Bottleneck-1.2.1.tar.gz (105kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 10.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting contextlib2>=0.4.0 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/71/8273a7eeed0aff6a854237ab5453bc9aa67deb49df4832801c21f0ff3782/contextlib2-0.5.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: networkx<2.0,>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from zipline==1.2.0->-r requirements.txt (line 15)) (1.11)\n",
      "Requirement already satisfied: numexpr>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from zipline==1.2.0->-r requirements.txt (line 15)) (2.6.4)\n",
      "Collecting bcolz<1,>=0.12.1 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/8b/1ffa01f872cac36173c5eb95b58c01040d8d25f1b242c48577f4104cd3ab/bcolz-0.12.1.tar.gz (622kB)\n",
      "\u001b[K    100% |████████████████████████████████| 624kB 13.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from zipline==1.2.0->-r requirements.txt (line 15)) (6.7)\n",
      "Collecting multipledispatch>=0.4.8 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "  Downloading https://files.pythonhosted.org/packages/89/79/429ecef45fd5e4504f7474d4c3c3c4668c267be3370e4c2fd33e61506833/multipledispatch-0.6.0-py3-none-any.whl\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from zipline==1.2.0->-r requirements.txt (line 15)) (1.0)\n",
      "Requirement already satisfied: Mako>=1.0.1 in /opt/conda/lib/python3.6/site-packages/Mako-1.0.7-py3.6.egg (from zipline==1.2.0->-r requirements.txt (line 15)) (1.0.7)\n",
      "Requirement already satisfied: sqlalchemy>=1.0.8 in /opt/conda/lib/python3.6/site-packages (from zipline==1.2.0->-r requirements.txt (line 15)) (1.1.13)\n",
      "Collecting alembic>=0.7.7 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/8b/0c98c378d93165d9809193f274c3c6e2151120d955b752419c7d43e4d857/alembic-1.0.11.tar.gz (1.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 11.9MB/s ta 0:00:01    41% |█████████████▎                  | 430kB 33.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sortedcontainers>=1.4.4 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "  Downloading https://files.pythonhosted.org/packages/13/f3/cf85f7c3a2dbd1a515d51e1f1676d971abe41bba6f4ab5443240d9a78e5b/sortedcontainers-2.1.0-py2.py3-none-any.whl\n",
      "Collecting intervaltree>=2.1.0 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/f9/76237755b2020cd74549e98667210b2dd54d3fb17c6f4a62631e61d31225/intervaltree-3.0.2.tar.gz\n",
      "Collecting lru-dict>=1.1.4 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "  Downloading https://files.pythonhosted.org/packages/00/a5/32ed6e10246cd341ca8cc205acea5d208e4053f48a4dced2b1b31d45ba3f/lru-dict-1.1.6.tar.gz\n",
      "Collecting empyrical>=0.4.2 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/55/a01b05162b764830dbbac868462f44cd847a5b6523a01ca9f955721819da/empyrical-0.5.0.tar.gz (49kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 7.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tables>=3.3.0 (from zipline==1.2.0->-r requirements.txt (line 15))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/f7/bb0ec32a3f3dd74143a3108fbf737e6dcfd47f0ffd61b52af7106ab7a38a/tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.3MB 6.4MB/s eta 0:00:01    23% |███████▋                        | 1.0MB 19.5MB/s eta 0:00:01    53% |█████████████████▏              | 2.3MB 28.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from osqp->cvxpy==1.0.3->-r requirements.txt (line 2)) (0.16.0)\n",
      "Collecting dill>=0.3.0 (from multiprocess->cvxpy==1.0.3->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/7a/70803635c850e351257029089d38748516a280864c97cbc73087afef6d51/dill-0.3.0.tar.gz (151kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 14.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.1 in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 6)) (4.3.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 6)) (2.6.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 6)) (4.4.0)\n",
      "Collecting requests-ftp (from pandas-datareader<0.6,>=0.2.1->zipline==1.2.0->-r requirements.txt (line 15))\n",
      "  Downloading https://files.pythonhosted.org/packages/3d/ca/14b2ad1e93b5195eeaf56b86b7ecfd5ea2d5754a68d17aeb1e5b9f95b3cf/requests-ftp-0.3.1.tar.gz\n",
      "Collecting python-editor>=0.3 (from alembic>=0.7.7->zipline==1.2.0->-r requirements.txt (line 15))\n",
      "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
      "Requirement already satisfied: mock>=2.0 in /opt/conda/lib/python3.6/site-packages (from tables>=3.3.0->zipline==1.2.0->-r requirements.txt (line 15)) (2.0.0)\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.6/site-packages (from mock>=2.0->tables>=3.3.0->zipline==1.2.0->-r requirements.txt (line 15)) (3.1.1)\n",
      "Building wheels for collected packages: cvxpy, plotly, zipline, scs, multiprocess, Logbook, cyordereddict, bottleneck, bcolz, alembic, intervaltree, lru-dict, empyrical, dill, requests-ftp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running setup.py bdist_wheel for cvxpy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2b/60/0b/0c2596528665e21d698d6f84a3406c52044c7b4ca6ac737cf3\n",
      "  Running setup.py bdist_wheel for plotly ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/98/54/81/dd92d5b0858fac680cd7bdb8800eb26c001dd9f5dc8b1bc0ba\n",
      "  Running setup.py bdist_wheel for zipline ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/5d/20/7d/b48368c8634b1cb6cc7232833b2780a265d4217c0ad2e3d24c\n",
      "  Running setup.py bdist_wheel for scs ... \u001b[?25l/"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helper\n",
    "import project_helper\n",
    "import project_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Data\n",
    "### Load Data\n",
    "While using real data will give you hands on experience, it's doesn't cover all the topics we try to condense in one project. We'll solve this by creating new stocks. We've create a scenario where companies mining [Terbium](https://en.wikipedia.org/wiki/Terbium) are making huge profits. All the companies in this sector of the market are made up. They represent a sector with large growth that will be used for demonstration latter in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('../../data/project_2/eod-quotemedia.csv', parse_dates=['date'], index_col=False)\n",
    "\n",
    "# Add TB sector to the market\n",
    "df = df_original\n",
    "df = pd.concat([df] + project_helper.generate_tb_sector(df[df['ticker'] == 'AAPL']['date']), ignore_index=True)\n",
    "\n",
    "close = df.reset_index().pivot(index='date', columns='ticker', values='adj_close')\n",
    "high = df.reset_index().pivot(index='date', columns='ticker', values='adj_high')\n",
    "low = df.reset_index().pivot(index='date', columns='ticker', values='adj_low')\n",
    "\n",
    "print('Loaded Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "To see what one of these 2-d matrices looks like, let's take a look at the closing prices matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Example\n",
    "Let's see what a single stock looks like from the closing prices. For this example and future display examples in this project, we'll use Apple's stock (AAPL). If we tried to graph all the stocks, it would be too much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_ticker = 'AAPL'\n",
    "project_helper.plot_stock(close[apple_ticker], '{} Stock'.format(apple_ticker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Alpha Research Process\n",
    "\n",
    "In this project you will code and evaluate a \"breakout\" signal. It is important to understand where these steps fit in the alpha research workflow. The signal-to-noise ratio in trading signals is very low and, as such, it is very easy to fall into the trap of _overfitting_ to noise. It is therefore inadvisable to jump right into signal coding. To help mitigate overfitting, it is best to start with a general observation and hypothesis; i.e., you should be able to answer the following question _before_ you touch any data:\n",
    "\n",
    "> What feature of markets or investor behaviour would lead to a persistent anomaly that my signal will try to use?\n",
    "\n",
    "Ideally the assumptions behind the hypothesis will be testable _before_ you actually code and evaluate the signal itself. The workflow therefore is as follows:\n",
    "\n",
    "![image](images/alpha_steps.png)\n",
    "\n",
    "In this project, we assume that the first three steps area done (\"observe & research\", \"form hypothesis\", \"validate hypothesis\"). The hypothesis you'll be using for this project is the following:\n",
    "- In the absence of news or significant investor trading interest, stocks oscillate in a range.\n",
    "- Traders seek to capitalize on this range-bound behaviour periodically by selling/shorting at the top of the range and buying/covering at the bottom of the range. This behaviour reinforces the existence of the range.\n",
    "- When stocks break out of the range, due to, e.g., a significant news release or from market pressure from a large investor:\n",
    "    - the liquidity traders who have been providing liquidity at the bounds of the range seek to cover their positions to mitigate losses, thus magnifying the move out of the range, _and_\n",
    "    - the move out of the range attracts other investor interest; these investors, due to the behavioural bias of _herding_ (e.g., [Herd Behavior](https://www.investopedia.com/university/behavioral_finance/behavioral8.asp)) build positions which favor continuation of the trend.\n",
    "\n",
    "\n",
    "Using this hypothesis, let start coding..\n",
    "## Compute the Highs and Lows in a Window\n",
    "You'll use the price highs and lows as an indicator for the breakout strategy. In this section, implement `get_high_lows_lookback` to get the maximum high price and minimum low price over a window of days. The variable `lookback_days` contains the number of days to look in the past. Make sure this doesn't include the current day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_lows_lookback(high, low, lookback_days):\n",
    "    \"\"\"\n",
    "    Get the highs and lows in a lookback window.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    high : DataFrame\n",
    "        High price for each ticker and date\n",
    "    low : DataFrame\n",
    "        Low price for each ticker and date\n",
    "    lookback_days : int\n",
    "        The number of days to look back\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lookback_high : DataFrame\n",
    "        Lookback high price for each ticker and date\n",
    "    lookback_low : DataFrame\n",
    "        Lookback low price for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    \n",
    "    lookback_high = high.shift().rolling(window = lookback_days).max()\n",
    "    lookback_low = low.shift().rolling(window = lookback_days).min()\n",
    "    \n",
    "    return lookback_high, lookback_low\n",
    "\n",
    "project_tests.test_get_high_lows_lookback(get_high_lows_lookback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's use your implementation of `get_high_lows_lookback` to get the highs and lows for the past 50 days and compare it to it their respective stock.  Just like last time, we'll use Apple's stock as the example to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_days = 50\n",
    "lookback_high, lookback_low = get_high_lows_lookback(high, low, lookback_days)\n",
    "project_helper.plot_high_low(\n",
    "    close[apple_ticker],\n",
    "    lookback_high[apple_ticker],\n",
    "    lookback_low[apple_ticker],\n",
    "    'High and Low of {} Stock'.format(apple_ticker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Long and Short Signals\n",
    "Using the generated indicator of highs and lows, create long and short signals using a breakout strategy. Implement `get_long_short` to generate the following signals:\n",
    "\n",
    "| Signal | Condition |\n",
    "|----|------|\n",
    "| -1 | Low > Close Price |\n",
    "| 1  | High < Close Price |\n",
    "| 0  | Otherwise |\n",
    "\n",
    "In this chart, **Close Price** is the `close` parameter. **Low** and **High** are the values generated from `get_high_lows_lookback`, the `lookback_high` and `lookback_low` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_long_short(close, lookback_high, lookback_low):\n",
    "    \"\"\"\n",
    "    Generate the signals long, short, and do nothing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close price for each ticker and date\n",
    "    lookback_high : DataFrame\n",
    "        Lookback high price for each ticker and date\n",
    "    lookback_low : DataFrame\n",
    "        Lookback low price for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    long_short : DataFrame\n",
    "        The long, short, and do nothing signals for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    \n",
    "    pos_one = (close > lookback_high).astype(np.int)\n",
    "    pos_two = -(close < lookback_low).astype(np.int)\n",
    "    long_short = pos_one+pos_two\n",
    "    \n",
    "    return long_short\n",
    "\n",
    "project_tests.test_get_long_short(get_long_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's compare the signals you generated against the close prices. This chart will show a lot of signals. Too many in fact. We'll talk about filtering the redundant signals in the next problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = get_long_short(close, lookback_high, lookback_low)\n",
    "project_helper.plot_signal(\n",
    "    close[apple_ticker],\n",
    "    signal[apple_ticker],\n",
    "    'Long and Short of {} Stock'.format(apple_ticker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Signals\n",
    "That was a lot of repeated signals! If we're already shorting a stock, having an additional signal to short a stock isn't helpful for this strategy. This also applies to additional long signals when the last signal was long.\n",
    "\n",
    "Implement `filter_signals` to filter out repeated long or short signals within the `lookahead_days`. If the previous signal was the same, change the signal to `0` (do nothing signal). For example, say you have a single stock time series that is\n",
    "\n",
    "`[1, 0, 1, 0, 1, 0, -1, -1]`\n",
    "\n",
    "Running `filter_signals` with a lookahead of 3 days should turn those signals into\n",
    "\n",
    "`[1, 0, 0, 0, 1, 0, -1, 0]`\n",
    "\n",
    "To help you implement the function, we have provided you with the `clear_signals` function. This will remove all signals within a window after the last signal. For example, say you're using a windows size of 3 with `clear_signals`. It would turn the Series of long signals\n",
    "\n",
    "`[0, 1, 0, 0, 1, 1, 0, 1, 0]`\n",
    "\n",
    "into\n",
    "\n",
    "`[0, 1, 0, 0, 0, 1, 0, 0, 0]`\n",
    "\n",
    "`clear_signals` only takes a Series of the same type of signals, where `1` is the signal and `0` is no signal. It can't take a mix of long and short signals. Using this function, implement `filter_signals`. \n",
    "\n",
    "For implementing `filter_signals`, we don't reccommend you try to find a vectorized solution. Instead, you should use the [`iterrows`](https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.iterrows.html) over each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_signals(signals, window_size):\n",
    "    \"\"\"\n",
    "    Clear out signals in a Series of just long or short signals.\n",
    "    \n",
    "    Remove the number of signals down to 1 within the window size time period.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signals : Pandas Series\n",
    "        The long, short, or do nothing signals\n",
    "    window_size : int\n",
    "        The number of days to have a single signal       \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    signals : Pandas Series\n",
    "        Signals with the signals removed from the window size\n",
    "    \"\"\"\n",
    "    # Start with buffer of window size\n",
    "    # This handles the edge case of calculating past_signal in the beginning\n",
    "    clean_signals = [0]*window_size\n",
    "    \n",
    "    for signal_i, current_signal in enumerate(signals):\n",
    "        # Check if there was a signal in the past window_size of days\n",
    "        has_past_signal = bool(sum(clean_signals[signal_i:signal_i+window_size]))\n",
    "        # Use the current signal if there's no past signal, else 0/False\n",
    "        clean_signals.append(not has_past_signal and current_signal)\n",
    "        \n",
    "    # Remove buffer\n",
    "    clean_signals = clean_signals[window_size:]\n",
    "\n",
    "    # Return the signals as a Series of Ints\n",
    "    return pd.Series(np.array(clean_signals).astype(np.int), signals.index)\n",
    "\n",
    "\n",
    "def filter_signals(signal, lookahead_days):\n",
    "    \"\"\"\n",
    "    Filter out signals in a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : DataFrame\n",
    "        The long, short, and do nothing signals for each ticker and date\n",
    "    lookahead_days : int\n",
    "        The number of days to look ahead\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filtered_signal : DataFrame\n",
    "        The filtered long, short, and do nothing signals for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    \n",
    "    long = (signal>0).astype(int).apply(lambda x : clear_signals(x, lookahead_days))\n",
    "    short = (signal<0).astype(int).apply(lambda x : clear_signals(x, lookahead_days))\n",
    "    \n",
    "    return long - short\n",
    "\n",
    "project_tests.test_filter_signals(filter_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's view the same chart as before, but with the redundant signals removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_5 = filter_signals(signal, 5)\n",
    "signal_10 = filter_signals(signal, 10)\n",
    "signal_20 = filter_signals(signal, 20)\n",
    "for signal_data, signal_days in [(signal_5, 5), (signal_10, 10), (signal_20, 20)]:\n",
    "    project_helper.plot_signal(\n",
    "        close[apple_ticker],\n",
    "        signal_data[apple_ticker],\n",
    "        'Long and Short of {} Stock with {} day signal window'.format(apple_ticker, signal_days))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookahead Close Prices\n",
    "With the trading signal done, we can start working on evaluating how many days to short or long the stocks. In this problem, implement `get_lookahead_prices` to get the close price days ahead in time. You can get the number of days from the variable `lookahead_days`. We'll use the lookahead prices to calculate future returns in another problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lookahead_prices(close, lookahead_days):\n",
    "    \"\"\"\n",
    "    Get the lookahead prices for `lookahead_days` number of days.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close price for each ticker and date\n",
    "    lookahead_days : int\n",
    "        The number of days to look ahead\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lookahead_prices : DataFrame\n",
    "        The lookahead prices for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return close.shift(-lookahead_days)\n",
    "\n",
    "project_tests.test_get_lookahead_prices(get_lookahead_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Using the `get_lookahead_prices` function, let's generate lookahead closing prices for 5, 10, and 20 days.\n",
    "\n",
    "Let's also chart a subsection of a few months of the Apple stock instead of years. This will allow you to view the differences between the 5, 10, and 20 day lookaheads. Otherwise, they will mesh together when looking at a chart that is zoomed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookahead_5 = get_lookahead_prices(close, 5)\n",
    "lookahead_10 = get_lookahead_prices(close, 10)\n",
    "lookahead_20 = get_lookahead_prices(close, 20)\n",
    "project_helper.plot_lookahead_prices(\n",
    "    close[apple_ticker].iloc[150:250],\n",
    "    [\n",
    "        (lookahead_5[apple_ticker].iloc[150:250], 5),\n",
    "        (lookahead_10[apple_ticker].iloc[150:250], 10),\n",
    "        (lookahead_20[apple_ticker].iloc[150:250], 20)],\n",
    "    '5, 10, and 20 day Lookahead Prices for Slice of {} Stock'.format(apple_ticker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookahead Price Returns\n",
    "Implement `get_return_lookahead` to generate the log price return between the closing price and the lookahead price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_return_lookahead(close, lookahead_prices):\n",
    "    \"\"\"\n",
    "    Calculate the log returns from the lookahead days to the signal day.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close price for each ticker and date\n",
    "    lookahead_prices : DataFrame\n",
    "        The lookahead prices for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lookahead_returns : DataFrame\n",
    "        The lookahead log returns for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return np.log(lookahead_prices) - np.log(close)\n",
    "\n",
    "project_tests.test_get_return_lookahead(get_return_lookahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Using the same lookahead prices and same subsection of the Apple stock from the previous problem, we'll view the lookahead returns.\n",
    "\n",
    "In order to view price returns on the same chart as the stock, a second y-axis will be added. When viewing this chart, the axis for the price of the stock will be on the left side, like previous charts. The axis for price returns will be located on the right side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_return_5 = get_return_lookahead(close, lookahead_5)\n",
    "price_return_10 = get_return_lookahead(close, lookahead_10)\n",
    "price_return_20 = get_return_lookahead(close, lookahead_20)\n",
    "project_helper.plot_price_returns(\n",
    "    close[apple_ticker].iloc[150:250],\n",
    "    [\n",
    "        (price_return_5[apple_ticker].iloc[150:250], 5),\n",
    "        (price_return_10[apple_ticker].iloc[150:250], 10),\n",
    "        (price_return_20[apple_ticker].iloc[150:250], 20)],\n",
    "    '5, 10, and 20 day Lookahead Returns for Slice {} Stock'.format(apple_ticker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Signal Return\n",
    "Using the price returns generate the signal returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signal_return(signal, lookahead_returns):\n",
    "    \"\"\"\n",
    "    Compute the signal returns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : DataFrame\n",
    "        The long, short, and do nothing signals for each ticker and date\n",
    "    lookahead_returns : DataFrame\n",
    "        The lookahead log returns for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    signal_return : DataFrame\n",
    "        Signal returns for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return signal*lookahead_returns\n",
    "\n",
    "project_tests.test_get_signal_return(get_signal_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's continue using the previous lookahead prices to view the signal returns. Just like before, the axis for the signal returns is on the right side of the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_string = '{} day LookaheadSignal Returns for {} Stock'\n",
    "signal_return_5 = get_signal_return(signal_5, price_return_5)\n",
    "signal_return_10 = get_signal_return(signal_10, price_return_10)\n",
    "signal_return_20 = get_signal_return(signal_20, price_return_20)\n",
    "project_helper.plot_signal_returns(\n",
    "    close[apple_ticker],\n",
    "    [\n",
    "        (signal_return_5[apple_ticker], signal_5[apple_ticker], 5),\n",
    "        (signal_return_10[apple_ticker], signal_10[apple_ticker], 10),\n",
    "        (signal_return_20[apple_ticker], signal_20[apple_ticker], 20)],\n",
    "    [title_string.format(5, apple_ticker), title_string.format(10, apple_ticker), title_string.format(20, apple_ticker)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for Significance\n",
    "### Histogram\n",
    "Let's plot a histogram of the signal return values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_helper.plot_signal_histograms(\n",
    "    [signal_return_5, signal_return_10, signal_return_20],\n",
    "    'Signal Return',\n",
    "    ('5 Days', '10 Days', '20 Days'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What do the histograms tell you about the signal returns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram are too small to make a definitive judgment but they seem to have a slightly positively skewed normal distribution.\n",
    "However the bump visible on the right sides of the histograms in the 10 days signal return and 20 days signal return cast doubt on the quality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "You might have noticed the outliers in the 10 and 20 day histograms. To better visualize the outliers, let's compare the 5, 10, and 20 day signals returns to normal distributions with the same mean and deviation for each signal return distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_helper.plot_signal_to_normal_histograms(\n",
    "    [signal_return_5, signal_return_10, signal_return_20],\n",
    "    'Signal Return',\n",
    "    ('5 Days', '10 Days', '20 Days'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kolmogorov-Smirnov Test\n",
    "While you can see the outliers in the histogram, we need to find the stocks that are causing these outlying returns. We'll use the Kolmogorov-Smirnov Test or KS-Test. This test will be applied to teach ticker's signal returns where a long or short signal exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out returns that don't have a long or short signal.\n",
    "long_short_signal_returns_5 = signal_return_5[signal_5 != 0].stack()\n",
    "long_short_signal_returns_10 = signal_return_10[signal_10 != 0].stack()\n",
    "long_short_signal_returns_20 = signal_return_20[signal_20 != 0].stack()\n",
    "\n",
    "# Get just ticker and signal return\n",
    "long_short_signal_returns_5 = long_short_signal_returns_5.reset_index().iloc[:, [1,2]]\n",
    "long_short_signal_returns_5.columns = ['ticker', 'signal_return']\n",
    "long_short_signal_returns_10 = long_short_signal_returns_10.reset_index().iloc[:, [1,2]]\n",
    "long_short_signal_returns_10.columns = ['ticker', 'signal_return']\n",
    "long_short_signal_returns_20 = long_short_signal_returns_20.reset_index().iloc[:, [1,2]]\n",
    "long_short_signal_returns_20.columns = ['ticker', 'signal_return']\n",
    "\n",
    "# View some of the data\n",
    "long_short_signal_returns_5.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives you the data to use in the KS-Test.\n",
    "\n",
    "Now it's time to implement the function `calculate_kstest` to use Kolmogorov-Smirnov test (KS test) between a distribution of stock returns (the input dataframe in this case) and each stock's signal returns. Run KS test on a normal distribution against each stock's signal returns. Use [`scipy.stats.kstest`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.kstest.html#scipy-stats-kstest) perform the KS test. When calculating the standard deviation of the signal returns, make sure to set the delta degrees of freedom to 0.\n",
    "\n",
    "For this function, we don't reccommend you try to find a vectorized solution. Instead, you should iterate over the [`groupby`](https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.groupby.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "\n",
    "\n",
    "def calculate_kstest(long_short_signal_returns):\n",
    "    \"\"\"\n",
    "    Calculate the KS-Test against the signal returns with a long or short signal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    long_short_signal_returns : DataFrame\n",
    "        The signal returns which have a signal.\n",
    "        This DataFrame contains two columns, \"ticker\" and \"signal_return\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ks_values : Pandas Series\n",
    "        KS static for all the tickers\n",
    "    p_values : Pandas Series\n",
    "        P value for all the tickers\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "\n",
    "    long_short_signal_returns_grouped = long_short_signal_returns.groupby('ticker')\n",
    "    \n",
    "    ks_values = pd.Series()\n",
    "    p_values = pd.Series()\n",
    "    args = (long_short_signal_returns.mean(), long_short_signal_returns.std(ddof=0))\n",
    "    for ticker, signal in long_short_signal_returns_grouped:\n",
    "            test_stat, p_value = kstest(signal['signal_return'].values, cdf='norm', args=args)\n",
    "            ks_values[ticker] = test_stat\n",
    "            p_values[ticker] = p_value\n",
    "    return ks_values, p_values\n",
    "\n",
    "project_tests.test_calculate_kstest(calculate_kstest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Using the signal returns we created above, let's calculate the ks and p values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_values_5, p_values_5 = calculate_kstest(long_short_signal_returns_5)\n",
    "ks_values_10, p_values_10 = calculate_kstest(long_short_signal_returns_10)\n",
    "ks_values_20, p_values_20 = calculate_kstest(long_short_signal_returns_20)\n",
    "\n",
    "print('ks_values_5')\n",
    "print(ks_values_5.head(10))\n",
    "print('p_values_5')\n",
    "print(p_values_5.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Outliers\n",
    "With the ks and p values calculate, let's find which symbols are the outliers. Implement the `find_outliers` function to find the following outliers:\n",
    "- Symbols that pass the null hypothesis with a p-value less than `pvalue_threshold`.\n",
    "- Symbols that with a KS value above `ks_threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(ks_values, p_values, ks_threshold, pvalue_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Find outlying symbols using KS values and P-values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ks_values : Pandas Series\n",
    "        KS static for all the tickers\n",
    "    p_values : Pandas Series\n",
    "        P value for all the tickers\n",
    "    ks_threshold : float\n",
    "        The threshold for the KS statistic\n",
    "    pvalue_threshold : float\n",
    "        The threshold for the p-value\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    outliers : set of str\n",
    "        Symbols that are outliers\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    \n",
    "    tickers = ((ks_values > ks_threshold) & (p_values < pvalue_threshold))\n",
    "    \n",
    "    outliers = set()\n",
    "    for i in range(len(tickers)):\n",
    "        if tickers[i] == True:\n",
    "            outliers.add(tickers.index[i])\n",
    "    return outliers\n",
    "\n",
    "project_tests.test_find_outliers(find_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Using the `find_outliers` function you implemented, let's see what we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_threshold = 0.8\n",
    "outliers_5 = find_outliers(ks_values_5, p_values_5, ks_threshold)\n",
    "outliers_10 = find_outliers(ks_values_10, p_values_10, ks_threshold)\n",
    "outliers_20 = find_outliers(ks_values_20, p_values_20, ks_threshold)\n",
    "\n",
    "outlier_tickers = outliers_5.union(outliers_10).union(outliers_20)\n",
    "print('{} Outliers Found:\\n{}'.format(len(outlier_tickers), ', '.join(list(outlier_tickers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Significance without Outliers\n",
    "Let's compare the 5, 10, and 20 day signals returns without outliers to normal distributions. Also, let's see how the P-Value has changed with the outliers removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tickers = list(set(close.columns) - outlier_tickers)\n",
    "\n",
    "project_helper.plot_signal_to_normal_histograms(\n",
    "    [signal_return_5[good_tickers], signal_return_10[good_tickers], signal_return_20[good_tickers]],\n",
    "    'Signal Return Without Outliers',\n",
    "    ('5 Days', '10 Days', '20 Days'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more like it! The returns are closer to a normal distribution. You have finished the research phase of a Breakout Strategy. You can now submit your project.\n",
    "## Submission\n",
    "Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade. You can continue to the next section while you wait for feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
